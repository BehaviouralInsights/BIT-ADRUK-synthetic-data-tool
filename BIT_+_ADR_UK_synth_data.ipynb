{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FHrGV7FGrWn"
      },
      "source": [
        "# Create a Low-Fidelity Synthetic Data Set\n",
        "\n",
        "This notebook was released with the report [Accelerating public policy research\n",
        "with synthetic data](https://www.adruk.org/news-publications/news-blogs/report-investigates-how-synthetic-data-can-be-used-in-government/).\n",
        "\n",
        "Dr. Paul Calcraft, Dr. Iorwerth Thomas, Martina Maglicic, Dr. Alex\n",
        "Sutherland\n",
        "\n",
        "Please contact iori.thomas@bi.team if you have any queries or suggestions.\n",
        "\n",
        "## Table of contents\n",
        "1. [Introduction](#introduction)\n",
        "2. [Load in your data set](#load_data)\n",
        "3. [Define relevant functions](#functions)\n",
        "4. [Generate synthetic data](#generate_data)\n",
        "\n",
        "## Introduction <a name=\"introduction\"></a>\n",
        "\n",
        "Synthetic data is artificially generated data that preserves the statistical properties of a data set while containing entirely different records. If used correctly, it represents less disclosure risk than the original data and could therefore be shared with fewer security requirements. This notebook allows you to create a synthetic version of your data set, with only minimum input from yourself. All you have to do is load in a data set of your choice and the script will do the rest. \n",
        "\n",
        "More specifically, the code will do three things: \n",
        "1. Extract information from the data set you've uploaded (e.g. the number of rows, whether a column contains numbers, structure etc.) \n",
        "2. Categorize each column according to its values (this can be numeric, categorical, datetime, string or NA)\n",
        "3. Generate new data that imitates key features of the original data set (so-called synthetic data)\n",
        "\n",
        "The only section that requires your input is [loading in the data set](#load_data), where you need to specify the name of the file you're uploading and the full path to the file. Once you've done that, you can run the script from start to finish and it should output a synthetic data set, which will be saved to your current working directory unless otherwise specified. If you wish, you can change the variable classification, for example if a variable has been categorized as string but it is in fact categorical - however, this is optional. \n",
        "\n",
        "As the synthetic data created with this script set preserves statistical properties incl. variable types, frequency of missing values and data set structure, but does not capture relationships between variables, the output data can be used for a variety of tasks. For example, it can be used as test data for analysis code (prior to the receipt of the original data), in order to train practitioners in how to use or analyse administrative data, for exploratory analysis or to familiarise any interested party with the data set and the questions it can answer. However, we recommend that you thoroughly inspect the resulting data set and check for (accidental) correlations prior to publishing it to ensure no sensitive information is being passed on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKAyIms4GrWv"
      },
      "source": [
        "## 1. Load in data set <a name=\"load_data\"></a>\n",
        "\n",
        "**This section requires your input.**\n",
        "\n",
        "We start by loading in necessary packages. For simplicity, we have limited dependencies to the minimum possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Automated package installation (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz_YCq_9GrWx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#install any necessary packages\n",
        "\"\"\"import sys\n",
        "!{sys.executable} -m pip install --user --upgrade setuptools\n",
        "!{sys.executable} -m pip install --user numpy==1.19\n",
        "!{sys.executable} -m pip install --user pandas>=0.24\n",
        "\n",
        "from packaging import version\n",
        "import pkg_resources\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "if version.parse(pd.__version__) < version.parse(\"1.0.5\"):\n",
        "    # require older numpy if we have old pandas\n",
        "    del pd\n",
        "    pkg_resources.require(\"numpy==1.19.0\")\n",
        "    pkg_resources.require(\"pandas>=0.23\") \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "from datetime import date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG3BRKh6GrWy"
      },
      "source": [
        "In order to load in your data set, please specify the file path. For example:\n",
        "\n",
        "file_path = \"C:\\Users\\FirstName.LastName\\Downloads\\General\\data.csv\" \n",
        "\n",
        "In the case that you keep the script in the same working directory as your data, you only need to specify the file name and extension. For example:\n",
        "\n",
        "file_path = \"data.csv\"\n",
        "\n",
        "Note that you should also provide a file name **without extension**, which will later be used to save the synthetic data set generated by the script (\"_synthetic\" will be automatically added to the file name). Else the file will be saved as \"_synthetic.csv\".\n",
        "\n",
        "Note that apart from Microsoft Excel files, only two dimensional data is currently supported for automatic upload and each column has to be a separate variable (as opposed to variables being contained in rows).\n",
        "\n",
        "**Please specify the file path below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpYrEv3oGrWz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 1) put in your full file path here, e.g. \"C:\\\\Users\\\\FirstName.LastName\\\\Downloads\\\\General\\\\data.csv\" \n",
        "\n",
        "file_path = \"\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODE = \"normal\"\n",
        "\n",
        "# This detects if you're using a .sav file -- if you have python3 it will attempt to preserve metadata from the file\n",
        "if (file_path.endswith('sav') and sys.version_info[0] == 3):\n",
        "    import pyreadstat\n",
        "    MODE = \"sav_file\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qAgxF9IGrW0"
      },
      "source": [
        "Once you have specified the file path and name above, you can run the entire script, which will automatically load the file, generate a synthetic data set and save it in your working directory.\n",
        "\n",
        "### Using a data pipeline\n",
        "\n",
        "Alternatively, if you are reading in the data from a database pipeline, uncomment the following: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#MODE = \"pipeline\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now uncomment the following lines and replace LIBRARY with the python library you use to access the pipeline, and PIPELINE_QUERY with the appropriate function for accessing the pipeline.  You will also need to define the name of the output file without an extension here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#include LIBRARY\n",
        "#original_data = PIPELINE_QUERY\n",
        "#file_name = \"OUTPUT_FILE_NAME\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modifying the output appearance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify how you want null or NaN values in your data to be represented in the synthetic data output\n",
        "null_string = \"\"\n",
        "\n",
        "# Specify precision of real numerical columns\n",
        "numerical_precision = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**The sections below do not require your input. Edit only if you would like to customize features.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCYFTjNZGrW0"
      },
      "source": [
        "## 2. Define relevant functions <a name=\"functions\"></a>\n",
        "\n",
        "This section defines all of the custom-made functions used later in the script to produce a synthetic data set (incl. the function used to classify variables and generate new data). It's worth reading this section if you want to get a deeper understanding of how the code works and what it does. \n",
        "\n",
        "### read_data()\n",
        "This function is used to read in your data. It detects the type of data you are trying to load (using the extension) and then picks out the correct pandas function to read in your data as a pandas dataframe. Currently 8 data types are supported: 'csv', 'txt', 'xlsx', 'xls', 'sas7bdat', 'sav', 'dta' and 'pkl'. Note that this function assumes the first row of data is a header."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNPlWloAGrW1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def read_data(x):\n",
        "    if (x.endswith(('csv', 'txt'))):\n",
        "        return pd.read_csv(x) \n",
        "    elif (x.endswith(('xlsx', 'xls'))):\n",
        "        dictionary = pd.read_excel(x, sheet_name = None) \n",
        "        if (len(dictionary.keys()) == 1):\n",
        "            name_of_sheet = list(dictionary.keys())\n",
        "            name_of_sheet = name_of_sheet[0]\n",
        "            simple_data_frame = dictionary[name_of_sheet]\n",
        "            return simple_data_frame\n",
        "        else:\n",
        "            return dictionary\n",
        "    elif (x.endswith('sas7bdat')):\n",
        "        return pd.read_sas(x) \n",
        "    elif (x.endswith('sav')):\n",
        "        return pd.read_spss(x) \n",
        "    elif (x.endswith('dta')):\n",
        "        return pd.read_stata(x)  \n",
        "    elif (x.endswith('pkl')):\n",
        "        return pd.read_pickle(x) \n",
        "    else:\n",
        "        raise Exception(\"Sorry, file type not supported. Try converting to csv, xlsx, txt or pkl.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uOnEJm6GrW1"
      },
      "source": [
        "### check_if_datetime()\n",
        "\n",
        "This function is used in the variable classification process. To check whether a variable contains only dates or times, we try to convert the column to datetime format using pandas, and if this yields no errors, we return 'True' else 'False'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAzPzQURGrW2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def check_if_datetime(x):\n",
        "    try:\n",
        "        pd.to_datetime(x, format='mixed', dayfirst=True)\n",
        "    except (RuntimeError, TypeError, NameError, IOError, ValueError):\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9UH1cqjGrW2"
      },
      "source": [
        "### check_if_numeric()\n",
        "\n",
        "To identify numeric variables, we try to convert columns to numeric using pandas, and if this yields no errors, we return 'True' else 'False'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-tKjr0XGrW2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define function\n",
        "def check_if_numeric(x):\n",
        "    try:\n",
        "        pd.to_numeric(x)\n",
        "    except (RuntimeError, TypeError, NameError, IOError, ValueError):\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQJTexqlGrW2"
      },
      "source": [
        "### identify_variable_type()\n",
        "\n",
        "With this function, we classify variables according to type. We check whether a variable is NA (i.e. empty), catgorical, numeric, datetime or string in the specified order. The function stops evaluating and assigns a type as soon as one of the below conditions is found to be true.\n",
        "\n",
        "- **NA** columns are columns that are empty, i.e. only have NA or null values.\n",
        "- **Categorical** columns have fewer than n number of unique values (n is either 100, or if the number of rows is very low, a third of the column length)\n",
        "- **Numeric** columns only have numeric values and values associated with numbers (e.g. a period or a minus), or are predominanly numeric but have very few unique values that do not fit the pattern. The latter is used to capture cases where NAs are represented by strings such as 'missing'.\n",
        "- **Datetime** columns are those columns that can be parsed into a datetime (meaning they are represented in a commonly accepted date format), or where most values can be parsed into datetime with the exception of a low number of unique values. Again, this is used to capture cases where NAs are represented by strings such as 'missing'.\n",
        "- Everything else is classified as **string**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFdf0iTAGrW3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def identify_variable_type(x):\n",
        "    # Is the column empty? If so, it will be classified as 'NA':\n",
        "    if (x.dropna().empty == True):\n",
        "        return \"NA\"\n",
        "    # Is the variable categorical? We check the number of unique values:\n",
        "    if ((x.dropna().shape[0] >= 300 and x.dropna().nunique()<100) or (x.dropna().nunique()<len(x)*0.3 and x.dropna().shape[0] < 300)):\n",
        "        return \"categorical\"\n",
        "    # If no numbers are present, we classify it as a string:\n",
        "    elif(x.astype(str).str.contains(r\"[0-9]\").any() == False):\n",
        "        return \"string\"\n",
        "    # We then check if it's numeric, or predominantly numeric with some exceptions:\n",
        "    elif(check_if_numeric(x) == True): \n",
        "        if x.min() > 19200101 and x.max() < 20260101:\n",
        "            return \"datetime\"\n",
        "        return \"numeric\"\n",
        "    elif(x.astype(str).str.contains(r\"[a-zA-Z]\").any() == True and \n",
        "         x[x.astype(str).str.contains(r\"[a-zA-Z]\")].nunique()<11 and \n",
        "         check_if_numeric(x[x.astype(str).str.contains(r\"[^a-zA-Z]\")]) == True):\n",
        "        return \"numeric\"\n",
        "    # next, we check if it's a date or a time, or predominantly datetime with some exceptions:\n",
        "    elif(check_if_datetime(x) == True):\n",
        "        return \"datetime\"\n",
        "    elif(x.astype(str)[x.astype(str).str.contains(r\"[0-9]\") == False].nunique() < 11 and\n",
        "         check_if_datetime(x[x.astype(str).str.contains(r\"[0-9]\") == True]) == True):\n",
        "        return \"datetime\"\n",
        "    # If none of the above apply, we classify the variable as string:\n",
        "    else:\n",
        "        return \"string\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjJZanowGrW4"
      },
      "source": [
        "### prepend(list, str)\n",
        "This is a simple function created for appending variable types to column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fPY68BmGrW4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def prepend(list, str): \n",
        "    str += '{0}'\n",
        "    list = [str.format(i) for i in list] \n",
        "    return(list) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKuqaqd9GrW4"
      },
      "source": [
        "### generate_datetime(min_time, max_time)\n",
        "\n",
        "This function is used in the process of generating synthetic datetime data. For simplicity, we assume a uniform distribution and randomly draw a timepoint that lies between the earliest and latest time recorded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwobzC40GrW5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def generate_datetime(min_time, max_time, to_floor = None):\n",
        "    start = pd.to_datetime(min_time)\n",
        "    end = pd.to_datetime(max_time)\n",
        "    random_date = start + (end - start) * random.random()\n",
        "    if to_floor:\n",
        "        return random_date.floor(to_floor)\n",
        "    return random_date\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrnjLj6wGrW5"
      },
      "source": [
        "### paste0(string, values)\n",
        "\n",
        "This function helps us create new column names and imitates the R function 'paste0' (essentially appending values to strings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBrWjm_5GrW5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def paste0(string, values):\n",
        "    texts = [string + str(num1) for num1 in values]\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCLw1lZzGrW5"
      },
      "source": [
        "### create_synthetic_data()\n",
        "\n",
        "This is the function for generates synthetic data. It assumes that variables have already been classified by type, and that these types have been appended to the column names. The function uses this information to create a similar data set; similar here means that variable type, values and/or patterns of individual variables will be preserved, but relationships between variables will not. Depending on variable type, this will entail a different procedure:\n",
        "- **NA**: We return an empty column.\n",
        "- **Numeric**: For simplicity, we assume that data points are normally distributed. We extract the mean and standard deviation from the variable and use this to simulate new data.\n",
        "- **Categorical**: We cross-tabulate categories and their associated frequencies, and convert these frequencies into probabilities that form a distribution from wich we simulate new data (e.g. if 'yes' and 'no' each appear 50% of the time in the existing column, then 'yes' and 'no' have a 50% chance of being drawn at each round as we generate new data points for the synthetic column).\n",
        "- **Datetime**: For simplicity, we assume that data points follow a uniform distribution and randomly draw time points that lie between the earliest and latest time recorded.\n",
        "- **String**: There are two possible outcomes depending on whether we detect a pattern. We assume a pattern is present if entries have roughly the same length, as a string with a pattern is likely to have similar lengths across entries (this will be the case for e.g. post codes and IP addresses, but not for case notes or customer reviews). If there is no pattern, we use a placeholder (‘sample text’) to create a new column. If a pattern is detected, we split each string into individual symbols and spread them across multiple columns by position (so the first symbol will be in the first column, the second symbol in the second column etc.) For each position, we compute the probability of each symbol occuring and then draw from the resulting distribution. This is done as many times as there are rows in the original data set. Finally, we merge the resulting symbols (previously split by position) into a string and return this new column as the final result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S47AZk1tGrW6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def create_synthetic_data(x):\n",
        "    # 1. Extract key information about the variable\n",
        "    nrow = x.shape[0]\n",
        "    nrow_NA = x.isnull().sum()\n",
        "    min_value = round(nrow_NA * 0.7)\n",
        "    max_value = nrow if nrow_NA * 1.3 > nrow else int(round(nrow_NA * 1.3))\n",
        "    nrow_NA = np.random.uniform(low = min_value, high = max_value)    \n",
        "    nrow_non_NA = int(round(nrow-nrow_NA))\n",
        "    # 2. Define procedure for empty columns (return an empty column)\n",
        "    if (x.name == None or x.name.endswith('NA')):\n",
        "        new_col = [np.nan for i in range(nrow)]\n",
        "        new_col = pd.Series(new_col)\n",
        "        return new_col\n",
        "    # 1. Define procedure for categorical variables\n",
        "    elif (x.name.endswith('categorical')):\n",
        "        # a) cross tab data (%)\n",
        "        cross_tab = x.value_counts(dropna = False, normalize=True)\n",
        "        # b) extract categories and frequencies\n",
        "        values = cross_tab.axes[0].tolist()\n",
        "        probs = cross_tab.tolist()\n",
        "        # c) create new column using numer of rows, categories and frequencies as input\n",
        "        new_col = np.random.choice(values, nrow, p=probs)\n",
        "        # d) turn array into pandas series and ensure NAs are displayes correctly\n",
        "        new_col = pd.Series(new_col).replace('nan', np.nan)\n",
        "        return new_col\n",
        "    # 2. Define procedure for numeric variables\n",
        "    elif(x.name.endswith('numeric')):\n",
        "         \n",
        "        def random_numerical_selection(x_mean, x_sd, nrow_non_NA, nrow):\n",
        "            probs=[nrow_non_NA/nrow, (nrow-nrow_non_NA)/nrow]\n",
        "            values=[np.random.normal(x_mean,x_sd), np.nan]\n",
        "            return np.random.choice(values, p=probs)\n",
        "        \n",
        "        # a) coerce to numeric \n",
        "        x = pd.to_numeric(x, errors = 'coerce')\n",
        "        is_integer = True if str(x.dtypes) == 'int64' or all(y.is_integer() or pd.isnull(y) for y in x) else False\n",
        "        # b) get mean and standard deviation\n",
        "        x_mean = x.mean()\n",
        "        x_sd = np.std(x)\n",
        "        # c) simulate data using normal distribution and proportion on NaNs\n",
        "        not_NAN_vals = nrow-x.isnull().sum()\n",
        "        new_col = [random_numerical_selection(x_mean, x_sd, nrow_non_NA, nrow) for _ in range(nrow)]\n",
        "        #new_col = np.random.normal(x_mean, x_sd, nrow_non_NA)\n",
        "        # d) turn array into pandas series and ensure NAs are displayes correctly\n",
        "        #new_col = np.pad(new_col, (0,int(nrow_NA)), \"constant\", constant_values=(np.nan,))\n",
        "        new_col = pd.Series(new_col).replace('nan', np.nan)\n",
        "        # e) ensure original format is respected (integer vs float)\n",
        "        if is_integer:\n",
        "            new_col = new_col.round(decimals=0)\n",
        "        #else:\n",
        "        #    new_col = round(new_col, num_precision)\n",
        "        # f) check if we have positive or negative values only (and correct if necessary)\n",
        "        nrow_negative_values = x[x < 0].dropna().shape[0] \n",
        "        nrow_positive_values = x[x > 0].dropna().shape[0] \n",
        "        if (nrow_negative_values > 0 and nrow_positive_values == 0):\n",
        "            new_col.loc[(new_col > 0)] = x.max()\n",
        "        if (nrow_positive_values > 0 and nrow_negative_values == 0):\n",
        "            new_col.loc[(new_col < 0)] = x.min()\n",
        "        \n",
        "        #e) ensure original format is respected (integer vs float)  -- put here or the rounding isn't imposed on some numbers\n",
        "        if (is_integer == True):\n",
        "            new_col = new_col.astype(pd.Int64Dtype(), errors='ignore')\n",
        "        else:\n",
        "            new_col = round(new_col, numerical_precision)\n",
        "        return new_col\n",
        "    # 3. Define procedure for datetime variables\n",
        "    elif(x.name.endswith('datetime')):\n",
        "        #a) coerce to datetime\n",
        "        x = pd.to_datetime(x.apply(str), errors = 'coerce')\n",
        "        #b) check whether it's date, time or datetime, and has nanoseconds\n",
        "        just_time = x.dt.time\n",
        "        no_ns = x.astype('datetime64[s]')\n",
        "        just_seconds = just_time.equals(no_ns.dt.time)\n",
        "        to_floor = 's' if just_seconds else None\n",
        "        just_time = just_time.dropna().astype(str)\n",
        "        # (if no times were in the original column, pandas will set it to midnight)\n",
        "        nrows_with_times = just_time[(just_time != '00:00:00')].shape[0] # if this is more than 0, then we have (some) times\n",
        "        just_date = x.dt.date\n",
        "        # (if no dates were in the original column, pandas will attach today's date)\n",
        "        todays_date = date.today() \n",
        "        todays_date = '{:%Y-%m-%d}'.format(todays_date)\n",
        "        just_date = just_date.dropna().astype(str)\n",
        "        nrows_with_dates = just_date[(just_date != todays_date)].shape[0] # if this is more than 0, then we have (some) dates\n",
        "        #b) get earliest and latest time points\n",
        "        t1 = min(x)\n",
        "        t2 = max(x)\n",
        "        #c) generate new data using a unform distribution\n",
        "        new_col = [generate_datetime(min_time = t1, max_time = t2, to_floor=to_floor) for i in range(nrow_non_NA)]\n",
        "        new_col = pd.Series(new_col)\n",
        "        # If this condition is met, we have datetime format:\n",
        "        if (nrows_with_times > 0 and nrows_with_dates > 0): \n",
        "            return new_col.replace('nan', np.nan)\n",
        "        # If this condition is met, we have just times\n",
        "        elif (nrows_with_times > 0 and nrows_with_dates == 0): \n",
        "            new_col = new_col.dt.time\n",
        "            return new_col.replace('nan', np.nan)\n",
        "        # Else, we have just dates:\n",
        "        else:  \n",
        "            new_col = new_col.dt.date\n",
        "            return new_col.replace('nan', np.nan)\n",
        "    # 4. Define procedure for string variables\n",
        "    elif(x.name.endswith('string')):\n",
        "        # 1) Compute essential information about the variable\n",
        "        x = x.astype(str)\n",
        "        av_character_length = x.dropna().apply(len).mean()\n",
        "        sd_character_length = x.dropna().apply(len).std()\n",
        "        max_character_length = x.dropna().apply(len).max()\n",
        "        min_character_length = x.dropna().apply(len).min()\n",
        "        # 2) Define a rule for determining whether a pattern exists \n",
        "        if (sd_character_length > 0.2*av_character_length):\n",
        "            # a) Return a placeholder for strings without patterns\n",
        "            def str_of_len(random_len):\n",
        "                unit = \"sample text \"\n",
        "                output = ''\n",
        "                while len(output) < random_len:\n",
        "                    output += unit\n",
        "                return f'{output[0:random_len-1]}'\n",
        "            \n",
        "            def random_length_str(max_len, min_len):\n",
        "                return str_of_len(np.random.default_rng().integers(min_len, max_len+1))\n",
        "                                           \n",
        "            #new_col = ['sample text' for i in range(nrow)]\n",
        "            new_col = [random_length_str(max_character_length,min_character_length) for i in range(nrow)]\n",
        "            new_col = pd.Series(new_col).replace('nan', np.nan)\n",
        "            return new_col\n",
        "        # If strings are similar in length, we assume that they have some meaningful pattern:\n",
        "        else: \n",
        "            # 3) Split the string column into multiple columns (one symbol per column)\n",
        "            x = x.dropna().apply(list)\n",
        "            new_col_names = paste0('position', range(1,(max_character_length+1)))\n",
        "            x_split = pd.DataFrame(x.tolist(), columns=[new_col_names])\n",
        "            # 4) Cross tabulate each column and extract values and associated frequencies\n",
        "            frequencies = x_split.apply(pd.Series.value_counts, normalize=True)\n",
        "            frequencies = frequencies.fillna(0)\n",
        "            # 5) Simulate data based on the probability of a certain value showing up in a certain position\n",
        "            result_df = pd.DataFrame()\n",
        "            # This for loop loops through all of the columns and draws an nrow number of values from each\n",
        "            for i in range(0, max_character_length): \n",
        "                result_col = np.random.choice(frequencies.axes[0].tolist(), nrow_non_NA, p=frequencies.iloc[:, i].tolist())\n",
        "                result_col = pd.Series(result_col)\n",
        "                result_df = pd.concat([result_df, result_col], axis=1, ignore_index = True)\n",
        "            # 6) Merge individual symbols into one string\n",
        "            result_df['all_values'] = result_df[result_df.columns[:]].apply(\n",
        "                lambda x: ''.join(x.dropna().astype(str)), axis=1)\n",
        "            # 7) Return result as a pandas series object\n",
        "            new_col = result_df['all_values'] \n",
        "            return new_col.replace('nan', np.nan)\n",
        "    # 5. If no type applies, return error message\n",
        "    else:\n",
        "        raise Exception(\"Error: No variable type specified. Variable types should be specified at the end of the variable name, e.g. 'gender_categorical'.\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OzhmzQ5GrW6"
      },
      "source": [
        "## 3. Generate Synthetic Data <a name=\"generate_data\"></a>\n",
        "\n",
        "This section uses the data set and functions specified above in order to generate synthetic data. If you'd like a detailed overview of what happens in each step, please take a look at the previous section, because this is where the features of individual functions are explained in depth. \n",
        "\n",
        "We start by loading in the specified data set as a pandas dataframe.\n",
        "\n",
        "Note that this function tries to infer column names using the first row of the data. If these are not column names (or if you'd like to skip some rows), please write your own code to load in your data. Otherwise, you might accidentally reveal an observation from the origina data set that has been preserved as column names when publishing the synthetic data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdGPLaFxGrW6",
        "outputId": "0bc02052-571a-44b2-d077-dc4faf23245a",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if MODE=='normal':\n",
        "    original_data = read_data(file_path)\n",
        "elif MODE=='sav_file':\n",
        "    original_data, metadata = pyreadstat.read_sav(file_path)\n",
        "\n",
        "original_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4s8QIzkGrXA"
      },
      "source": [
        "In case the data set is a Microsoft Excel file with multiple sheets, the data will have been imported as a **dictionary** that holds all of the sheets as separate data sets. This means we have to loop our code through the sheets. If this is the case, we run the below code (it automatically checks if the data is in dictionary format) and stop executing the script as the code that follows has been written for two dimensional data. You might see a warning 'SystemExit: 0', but this just means the code is working as expected, and your synthetic data set has already been saved in your working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViDE6wStGrXB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# check if we have a data set in dictionary format\n",
        "if (type(original_data) == dict):\n",
        "    # create excel writer to save output\n",
        "    base = os.path.basename(file_path)\n",
        "    file_name = os.path.splitext(base)[0] \n",
        "    writer = pd.ExcelWriter(file_name + '_synthetic' + '.xlsx') \n",
        "    # loop code through each dictionary item (sheet in spreadsheet)\n",
        "    for sheet in original_data:\n",
        "        # save original col names\n",
        "        original_sheet = original_data[sheet]\n",
        "        original_column_names = list(original_sheet.columns)\n",
        "        original_column_names = list(map(str, original_column_names))\n",
        "        #trim trailing white space\n",
        "        original_sheet = original_sheet.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "        #3) Classify and rename columns\n",
        "        column_types = original_sheet.apply(identify_variable_type)\n",
        "        column_types = prepend(column_types, '_')\n",
        "        new_column_names = [i + j for i, j in zip(original_column_names, column_types)] \n",
        "        original_sheet.columns = new_column_names\n",
        "        # Generate new data\n",
        "        synthetic_data = original_sheet.apply(create_synthetic_data)\n",
        "        # Rename columns back to original names\n",
        "        synthetic_data.columns = original_column_names\n",
        "        # Save sheet to file\n",
        "        synthetic_data.to_excel(writer, sheet_name = sheet, index = False)\n",
        "    # Save excel file once all sheets are done    \n",
        "    writer.save()\n",
        "    print('Success! Your synthetic data set has been saved as ' + file_name + '_synthetic' + '.xlsx' + ' in your working directory.')\n",
        "    # Stop executing script\n",
        "    sys.exit(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGvRtxGGGrXB"
      },
      "source": [
        "If the data is **not in dictionary format**, we run code below instead.\n",
        "\n",
        "We start by saving the original column names. The next step is trimming trailing white space, as this may otherwise prevent correct variable classifiction. We subsequently classify variables according to type (NA, numeric, catgeorical, string or datetime) and append the type to the original column name. This will later be used to pass information about the variable type to the next step. The reason variables are classified and new data generated in two separate steps is to allow for correction and customization of variable types. At the end of this code chunk, you'll see how variables have been classified and you can check whether you're happy with it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQsvAczWGrXC",
        "outputId": "8773c235-30ea-439e-9dba-8f42e189cf66",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# save original column names\n",
        "original_column_names = list(original_data.columns)\n",
        "\n",
        "# clean column names\n",
        "#original_data.columns = original_data.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
        "\n",
        "#trim trailing white space\n",
        "for column in original_data.columns: # written to optimise memory\n",
        "    column_data = original_data[[column]].copy(deep=True)\n",
        "    column_data = column_data.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "    #cols = original_data.select_dtypes(['object']).columns\n",
        "    if column_data[column].dtype == object:\n",
        "        column_data = column_data.astype(str).apply(lambda x: x.str.strip())\n",
        "    original_data[column]=column_data\n",
        "    del [[column_data]] #delete working column from memory\n",
        "    gc.collect() #garbage collect to make sure it's gone\n",
        "\n",
        "# Classify and rename columns\n",
        "column_types = original_data.apply(identify_variable_type)\n",
        "column_types = prepend(column_types, '_')\n",
        "new_column_names = [i + j for i, j in zip(list(original_data.columns), column_types)] \n",
        "original_data.columns = new_column_names\n",
        "\n",
        "new_column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f01WUUmJGrXD"
      },
      "source": [
        "**Correction of Variable Classification (optional)**\n",
        "\n",
        "Please check the variable classification shown above. If you are unhappy and would like to change the classification, you can do so below. Simply remove the the hashtags and insert the right column names. \n",
        "\n",
        "\"column_to_be_renamed\" refers to the current name of the column you'd like to rename. This should consist of the original column name with an underscore and a variable type added at the end, e.g.: \"county_categorical\"\n",
        "\n",
        "\"new_column_name\" refers to the new column name. Simply replace the wrong variable type with the correct one. Possible variable types are 'string', 'categorical', 'numeric', 'NA' or 'datetime'. For example, we could change \"county_categorical\" into \"county_string\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsbbwwGmGrXD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# original_data = original_data.rename({\"column_to_be_renamed1\":\"new_column_name1\",\"column_to_be_renamed2\":\"new_column_name2\"}, axis='columns') \n",
        "# original_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcB-KJx0GrXE"
      },
      "source": [
        "In a final step, we generate synthetic data. This data set will have the same column names, structure and number of rows as your original data set. Frequency of NAs values will be preserved (albeit with the introduction of some noise). Values found in columns will be largely similar (e.g. similar structure of string values, similar frequency of categories for categorical values etc). The new data set will have 'synthetic' appended to its file name and saved as a .csv file in your current working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z2A6L7tGrXE",
        "outputId": "dd585922-0e4c-4366-dce8-3dcf28414662",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Generate new data: here we do everything column by column and overwrite original dataframe to save memory\n",
        "for column in original_data.columns:\n",
        "    column_data = original_data[[column]].copy(deep=True)\n",
        "    original_data[column] = column_data.apply(create_synthetic_data)\n",
        "    del [[column_data]] # delete column data to save memory\n",
        "    gc.collect() #force garbage collection to clear column data from memory\n",
        "    \n",
        "# Rename columns\n",
        "original_data.columns = original_column_names\n",
        "\n",
        "# Replace nulls/NaNs with string\n",
        "for column in original_data.columns:\n",
        "    if original_data[column].dtype != pd.Int64Dtype():\n",
        "        original_data[column] = original_data[column].replace(np.nan, null_string)\n",
        "\n",
        "# Work out file_name from path (if not using a pipeline)\n",
        "if MODE != 'pipeline':\n",
        "    base = os.path.basename(file_path)\n",
        "    file_name = os.path.splitext(base)[0] \n",
        "\n",
        "data_location = file_name + '_synthetic'\n",
        "\n",
        "# save to csv\n",
        "original_data.to_csv(data_location + '.csv', index = False)\n",
        "# save to sav format if original is .sav\n",
        "if MODE == 'sav_file':\n",
        "    pyreadstat.write_sav(original_data, data_location + '.sav', file_label=metadata.file_label, column_labels=metadata.column_labels,  variable_value_labels=metadata.variable_value_labels,missing_ranges = metadata.missing_ranges, variable_display_width=metadata.variable_display_width, variable_measure=metadata.variable_measure )\n",
        "\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le6HBFZBGrXE"
      },
      "source": [
        "**Please thoroughly inspect your data before sharing it publicly.** While the code has been written to minimise the likelihood of sharing sensitive data, we cannot guarantee this will always be the case. Therefore, we recommend checking for (accidental) correlations between variables, observations from the original data set that might have been reproduced in the synthetic data set by chance, or any other revealing information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Licence Information\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2022 Behavioural Insights Team\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BIT + ADR UK: Generate simple synthetic data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
